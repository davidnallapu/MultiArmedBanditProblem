# MultiArmedBanditProblem
The multi-armed bandit problem is to maximize our reward by balancing exploration and exploitation of allocating limited resources to competing choices. The word 'bandit' comes from modeling this problem after slot machines in casinos. This study aims to understand and solve the multi-armed bandit problem with Thompson Sampling, UCB and  ùúñ -greedy. Random Sampling will also be used to create a benchmark to understand the performance of the other algorithm. Andre Cianflone's notebook[3] compares and random sampling. This study extends Andre Cianflone's notebook which compared Thompson Sampling, e-greedy, UBC to answer questions about hyperparameters, action space, stationarity, changing the distribustion for Thompson Sampling to Gaussian and benchmarking with random sampling. We will see the advantages and disadvantages of various algorithms. The Thompson algorithm has been shown to perform quite well in various settings without any necessary parameter tuning to boot. In some cases, UCB performed close to Thompson, but required fine tuning. Exceptionally, UCB dealt better than Thompson in the non-stationary setup, but only after tuning its hyperparameter. Notably, in no cases was ùúñ -greedy the best performing agent. The findings from this study indicate that it is important to compare various algorithms for the multi-armed bandit problem as they perform differently in varied environments.

Conclusion
Thompson Sampling seems to to do the best in most experiments with a few exemptions. Thompson Sampling with the Beta distribution seems to be the best option. UCB does outperform Thompson Sampling in the non-stationarity experiment and with a large action space but there is a huge emphasis on its hyperparameter 'c' for its stability. E-greedy on the other hand is a lot more stable than UCB and performs the best in less timestamps and in a large action space.
